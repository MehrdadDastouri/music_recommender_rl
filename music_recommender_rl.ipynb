{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMw1ktTeWX9lumxNK00x52x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadDastouri/music_recommender_rl/blob/main/music_recommender_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MusicEnvironment:\n",
        "    \"\"\"Simulated environment for music recommendation.\"\"\"\n",
        "\n",
        "    def __init__(self, music_data: pd.DataFrame):\n",
        "        self.music_data = music_data\n",
        "        self.current_state = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.features = self._prepare_features()\n",
        "\n",
        "    def _prepare_features(self) -> np.ndarray:\n",
        "        \"\"\"Prepare and normalize music features.\"\"\"\n",
        "        features = self.music_data[['danceability', 'energy', 'valence', 'tempo']].values\n",
        "        return self.scaler.fit_transform(features)\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"Reset environment to initial state.\"\"\"\n",
        "        self.current_state = np.random.randint(len(self.features))\n",
        "        return self.features[self.current_state]\n",
        "\n",
        "    def step(self, action: int) -> Tuple[np.ndarray, float, bool]:\n",
        "        \"\"\"Take action and return new state, reward and done flag.\"\"\"\n",
        "        next_state = action\n",
        "        reward = self._calculate_reward(self.current_state, next_state)\n",
        "        self.current_state = next_state\n",
        "        return self.features[next_state], reward, False\n",
        "\n",
        "    def _calculate_reward(self, current: int, next: int) -> float:\n",
        "        \"\"\"Calculate reward based on music similarity and user preferences.\"\"\"\n",
        "        similarity = self._compute_similarity(self.features[current], self.features[next])\n",
        "        return float(similarity)\n",
        "\n",
        "    def _compute_similarity(self, state1: np.ndarray, state2: np.ndarray) -> float:\n",
        "        \"\"\"Compute similarity between two music states.\"\"\"\n",
        "        return 1.0 / (1.0 + np.linalg.norm(state1 - state2))\n",
        "\n",
        "class DQNNetwork(nn.Module):\n",
        "    \"\"\"Deep Q-Network for music recommendation.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size: int, action_size: int):\n",
        "        super(DQNNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, action_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class MusicRecommender:\n",
        "    \"\"\"Music recommendation system using reinforcement learning.\"\"\"\n",
        "\n",
        "    def __init__(self, music_data: pd.DataFrame):\n",
        "        self.env = MusicEnvironment(music_data)\n",
        "        self.state_size = 4  # number of features\n",
        "        self.action_size = len(music_data)\n",
        "        self.memory = []\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = DQNNetwork(self.state_size, self.action_size).to(self.device)\n",
        "        self.target_model = DQNNetwork(self.state_size, self.action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters())\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        \"\"\"Update target network with weights from main network.\"\"\"\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state: np.ndarray, action: int, reward: float,\n",
        "                next_state: np.ndarray, done: bool):\n",
        "        \"\"\"Store experience in memory.\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state: np.ndarray) -> int:\n",
        "        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            action_values = self.model(state_tensor)\n",
        "        return action_values.argmax().item()\n",
        "\n",
        "    def replay(self, batch_size: int):\n",
        "        \"\"\"Train the model using experience replay.\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "\n",
        "        for idx in minibatch:\n",
        "            s, a, r, ns, d = self.memory[idx]\n",
        "            states.append(s)\n",
        "            actions.append(a)\n",
        "            rewards.append(r)\n",
        "            next_states.append(ns)\n",
        "            dones.append(d)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "\n",
        "        current_q_values = self.model(states)\n",
        "        next_q_values = self.target_model(next_states).detach()\n",
        "\n",
        "        target_q_values = current_q_values.clone()\n",
        "        for i in range(batch_size):\n",
        "            target_q_values[i][actions[i]] = rewards[i] + \\\n",
        "                (1 - dones[i]) * self.gamma * next_q_values[i].max()\n",
        "\n",
        "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self, episodes: int = 1000):\n",
        "        \"\"\"Train the recommendation system.\"\"\"\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "\n",
        "            for _ in range(100):  # max steps per episode\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done = self.env.step(action)\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "                self.replay(self.batch_size)\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            if episode % 10 == 0:\n",
        "                self.update_target_model()\n",
        "                print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {self.epsilon}\")\n",
        "\n",
        "    def recommend(self, current_song_idx: int, n_recommendations: int = 5) -> List[int]:\n",
        "        \"\"\"Get music recommendations for a given song.\"\"\"\n",
        "        state = self.env.features[current_song_idx]\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action_values = self.model(state_tensor)\n",
        "\n",
        "        return action_values.squeeze().argsort(descending=True)[:n_recommendations].cpu().numpy()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create sample music data\n",
        "    music_data = pd.DataFrame({\n",
        "        'danceability': np.random.random(100),\n",
        "        'energy': np.random.random(100),\n",
        "        'valence': np.random.random(100),\n",
        "        'tempo': np.random.random(100),\n",
        "        'title': [f'Song_{i}' for i in range(100)]\n",
        "    })\n",
        "\n",
        "    # Initialize and train recommender\n",
        "    recommender = MusicRecommender(music_data)\n",
        "    recommender.train(episodes=100)\n",
        "\n",
        "    # Get recommendations\n",
        "    current_song = 0\n",
        "    recommendations = recommender.recommend(current_song)\n",
        "    print(\"\\nRecommendations for\", music_data.iloc[current_song]['title'])\n",
        "    for idx in recommendations:\n",
        "        print(music_data.iloc[idx]['title'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnXHkqCljbi5",
        "outputId": "24b24c34-d6ef-4c25-adb8-6ce5bc67e82b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Total Reward: 29.1053869128023, Epsilon: 0.7076077347272662\n",
            "Episode: 10, Total Reward: 97.95737142637364, Epsilon: 0.00998645168764533\n",
            "Episode: 20, Total Reward: 99.45622802606616, Epsilon: 0.00998645168764533\n",
            "Episode: 30, Total Reward: 96.23949164185713, Epsilon: 0.00998645168764533\n",
            "Episode: 40, Total Reward: 99.28784503446326, Epsilon: 0.00998645168764533\n",
            "Episode: 50, Total Reward: 96.5823917727028, Epsilon: 0.00998645168764533\n",
            "Episode: 60, Total Reward: 97.09234997302008, Epsilon: 0.00998645168764533\n",
            "Episode: 70, Total Reward: 95.68002766183787, Epsilon: 0.00998645168764533\n",
            "Episode: 80, Total Reward: 90.42511847923537, Epsilon: 0.00998645168764533\n",
            "Episode: 90, Total Reward: 98.05456995741442, Epsilon: 0.00998645168764533\n",
            "\n",
            "Recommendations for Song_0\n",
            "Song_0\n",
            "Song_12\n",
            "Song_43\n",
            "Song_61\n",
            "Song_26\n"
          ]
        }
      ]
    }
  ]
}